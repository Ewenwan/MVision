
中文教程 
https://zh.gluon.ai/

mxnet的gluon

GAN生成模型
http://www.jianshu.com/u/a24acc5449c7


官方安装方法
http://mxnet.incubator.apache.org/get_started/install.html


http://www.cnblogs.com/daihengchen/p/5924768.html

视频教程
http://space.bilibili.com/209599371#!/


1、安装依赖 最新版本的pip
sudo apt-get update
sudo apt-get install -y wget python gcc
wget https://bootstrap.pypa.io/get-pip.py && sudo python get-pip.py

2、安装MXnet
sudo pip install mxnet==0.11.0
一般在/usr/local/lib/python2.7/dist-packages/mxnet

GPU版本
sudo pip install mxnet-cu75 --pre --user   # for CUDA 7.5 
sudo pip install mxnet-cu80 --pre --user   # for CUDA 8.0

3、安装可视化工具（可选） mxnet.viz
sudo apt-get install graphviz
sudo pip install graphviz

4、安装 Intel MKL
sudo pip install mxnet-mkl==0.11.0

5、验证安装
python

>>> import mxnet as mx
>>> a = mx.nd.ones((2, 3))
>>> b = a * 2 + 1
>>> b.asnumpy()
array([[ 3.,  3.,  3.],
       [ 3.,  3.,  3.]], dtype=float32)

>>> exit()

6、源码安装
sudo apt-get update
sudo apt-get install -y build-essential git libatlas-base-dev libopencv-dev
git clone --recursive https://github.com/dmlc/mxnet
cd mxnet; make -j4

出现错误
/usr/bin/ld: cannot find -lcblas
安装
sudo apt-get install libatlas-base-dev


添加搜索路径
// ~表示我的mxnet安装在home下面
export PYTHONPATH=~/mxnet/python
cp -r ~/mxnet/python/mxnet .
cp ~/mxnet/lib/libmxnet.so mxnet/

//我的工作目录
export PYTHONPATH=/home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/python
cp -r /home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/python/mxnet .
cp -r /home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/lib/libmxnet.so mxnet/

本次对话生效
sudo vi /etc/profile
在最后添加

export PYTHONPATH=/home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/python

上述方法的PATH 在终端关闭 后就会消失。所以还是建议通过编辑/etc/profile来改PATH，
也可以改家目录下的.bashrc(即：~/.bashrc)。
 

第二种方法：
# vim /etc/profile
在文档最后，添加:
export PYTHONPATH=/home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/python
保存，退出，然后运行：
#source /etc/profile

不报错则成功。

第三种
vim ~/.bashrc
在文档最后，添加:
export PYTHONPATH=/home/ewenwan/ewenwan/learn/vSLAM/test/MXnet/mxnet/python


7、github教程
https://github.com/dmlc/mxnet-notebooks


http://blog.csdn.net/shuzfan/article/details/50037273

8、转换图片格式
就好像在caffe中训练模型之前要现将训练图片转化为leveldb或者LMDB格式数据一样，
mxnet在训练之前也要先将图片转化为REC格式的数据。

linux下转换使用的工具是mxnet/bin下名为“im2rec”的可执行文件，
该文件的源码可以查看mxnet/tools/im2rec.cc  or mxnet/tools/im2rec.py

首先，来说明以下im2rec这个转换工具有哪些输入参数：

【1】首先是必须输入的3个参数，依次是
image_lst:      //保存图片和标签信息的文本的名字
image_root_dir：//图片的存放路径
output_rec：    //转换后的.rec格式文件的存放路径


【2】可选的输入参数有：
label_width: //标签个数，默认为1，即每个图片只有一个标签
resize:      //默认为-1，即不调整图片尺寸。否则将图片的较短边的长度调整为resize大小，另一边按比例调整
%%例如图片尺寸为300*200，resize=100,那么调整之后图片尺寸为150*100.
quality：    //JPEG编码质量，默认为80。输入范围为1-100.（mxnet转换数据时是编码存储的）
center_crop：//默认为0，if(center_crop)，则将图片以中心裁剪为正方形。注意：只有if(resize>0)成立时，该参数才有效。
nsplit：     //默认为1，数据集分块 用于将部分图片进行转换，按照位置将image_lst中的图片分成nsplit个part
    %%注意，这里只是大概平分，例如12个训练图片，nsplit=3，实际分块为5+4+3，而不是预想中的4+4+4.（关于这一点我也还没搞明白）
part： //默认为0，用于将部分图片进行转换。和nsplit结合使用，可以只转换nsplit中的第part个分块

【3】这里举一个实际应用的例子
（1）首先是保存图片和标签信息的文本，这里我的文件是train.txt.
编号 标签 图片名
1 0 FY2D1.bmp
2 1 FY2E1.bmp
3 0 FY2D2.bmp
4 1 FY2E2.bmp

python im2rec.py train.txt data/ train.rec quality=90 resize=1000


9、迭代器
迭代器-将转换后的数据送入网络:

总体来说，开始一项训练，需要3个部分：训练参数、网络模型和训练数据。
在上一小节，我已经详细介绍了如何将原始图片数据转化为mxnet可以接受的REC格式数据。
但是，实际训练的时候我们还需要一个工具来将REC数据送入我们的训练网络中。
这个工具就是“迭代器”-iterator


【1】mxnet为我们提供了快速构建迭代器的函数ImageRecordIter，该函数位于python/mxnet/io.py中，
其返回一个迭代器对象。或者可以在python命令窗中键入以下命令来获取io函数的位置以及说明：

import mxnet as mx
help(mx.io)
#或者输入help(mx.io.ImageRecordIter)来直接查看迭代器的参数说明


【2】接下来说明ImageRecordIter的几个常用参数：（更多的参数可以通过上面的help查看）

path_imgrec：rec图像数据的存储路径；string型数据；默认为'./data/imgrec.rec'
data_shape：迭代器产生的每一个实例的形状，即图片数据的形状；元组型数据；必选参数
mean_img：图像均值文件的存放路径；string型数据；默认为空
batch_size：batch的大小（一次训练的图片数量）；int型数据；必选参数
shuffle：是否打乱数据；布尔型数据；默认为False;
mirror：是否对图片执行镜像操作；布尔型数据；默认为False;
rand_mirror：是否随机地对图片执行镜像操作；布尔型数据；默认为False;
rand_crop：是否随机地对图片执行剪切操作；布尔型数据；默认为False;
label_width：图片的标签个数；int型数据；默认为1
mean_r、 mean_g、 mean_b:3个颜色通道的均值；float型数据；默认为0
num_parts：将训练数据分为几份；int型数据；默认为1
part_index:第几部分数据将会被读取；int型数据；默认为0


【3】接下来以实例来说明怎样构建迭代器，
该例子位于example/image-classification/train_imagenet.py中

#分别为训练数据和验证数据各构建一个迭代器
def get_iterator(args, kv):
    train = mx.io.ImageRecordIter(
        path_imgrec = args.data_dir + "train.rec",
        mean_img    = args.data_dir + "mean.bin",
        data_shape  = data_shape,
        batch_size  = args.batch_size,
        rand_crop   = True,
        rand_mirror = True,
        num_parts   = kv.num_workers,
        part_index  = kv.rank)
    val = mx.io.ImageRecordIter(
        path_imgrec = args.data_dir + "val.rec",
        mean_img    = args.data_dir + "mean.bin",
        rand_crop   = False,
        rand_mirror = False,
        data_shape  = data_shape,
        batch_size  = args.batch_size,
        num_parts   = kv.num_workers,
        part_index  = kv.rank)
    return (train, val)


10 构建一个网络模型
一个网络模型是一层一层连接器来的，因此想要构建一个网络模型，
首先我们得知道mxnet中都有哪些可以用的层。(以下按照使用频率来排序)
下面介绍一些比较常用的层，更多的层大家可以在python命令窗执行以下代码来查询
import mxnet as mx
help(mx.symbol)
【1】Activation：激活函数
   data: 输入数据，symbol型变量
   act_type: {'relu', 'sigmoid', 'tanh'}, 可选的3种激活函数

【2】Convolution:卷积层
data:输入数据，symbol型变量
weight：权重矩阵，symbol型变量
bias:偏置，symbol型变量
kernal:卷积核的尺寸（y,x）,二维tuple型变量
stride：卷积步长（y,x）,二维tuple型变量，默认为（1,1）
pad：卷积的填补量（y,x）,二维tuple型变量，默认为（0,0）
num_filter ：卷积滤波器(通道)个数，int型变量
no_bias ：是否取消偏置，布尔型变量，默认为False
num_group：groups划分的个数，int型变量，默认为1.
  该选项不被CUDNN所支持， you can use SliceChannel to num_group,
  apply convolution and concat instead to achieve the same need.
workspace：卷积的Tmp工作空间的大小MB。long型变量，默认为512


【3】Deconvolution：反卷积层

   参数同卷积层一致

【4】Dropout： 神经元失效层
 data:输入数据，symbol型变量
 p : 训练时对输入数据进行dropped out的比例，float型变量，默认为0.5

【5】Flatten:将N维数据变为1维

data:输入数据，symbol型变量
name : string, optional. Name of the resulting symbol.

【6】FullyConnected：全连接层
data:输入数据，symbol型变量
weight：权重矩阵，symbol型变量
bias:偏置，symbol型变量
no_bias ：是否取消偏置，布尔型变量，默认为False
num_hidden：输出隐藏节点的个数，int型变量


【7】LRN：Local Response Normalization，即局部响应归一化层
data:输入数据，symbol型变量
alpha : float, optional, default=0.0001.value of the alpha variance scaling parameter in the normalization formula
beta : float, optional, default=0.75.value of the beta power parameter in the normalization formula
knorm : float, optional, default=2.value of the k parameter in normalization formula
nsize : int (non-negative), required.normalization window width in elements.

【8】Pooling 池化层

data:输入数据，symbol型变量
kernal:pooling核的尺寸（y,x）,二维tuple型变量
stride：pooling步长（y,x）,二维tuple型变量，默认为（1,1）
pad：pooling的填补量（y,x）,二维tuple型变量，默认为（0,0）
pool_type : pooling的类型，可选的3种{'avg', 'max', 'sum'}


【9】SoftmaxOutput：使用logloss进行BP   最后回归输出

data:输入数据，symbol型变量
grad_scale : 为梯度乘以一个比例系数，float型变量，默认为1
multi_output : 布尔型变量，默认为False
    # If set to true, for a (n,k,x_1,..,x_n) dimensionalinput tensor, softmax will generate n*x_1*...*x_n output, eachhas k classes

【10】Variable：定义一个symbol型变量

name : 变量的名字，str型变量
attr : dict of string -> string.Additional attributes to set on the variable.

mxnet/example/image-classification文件夹下以symbol开头的几个.py文件都是定义好的网络模型。
我们可以任意打开一个来具体了解一下如何构建一个网络。
有一点需要注意的是：mxnet中通常要在卷积层和全连接层之间价加一个Flatten层用于衔接


11 开始训练
首先，来总结一下我们现在都有了什么：
保存网络模型的.py文件、
REC格式的训练数据、
将数据送入网络的迭代器。

在此基础上，我们缺少的就只剩下网络训练的一些控制参数：如学习率、训练日志、GPU选择等等。
在初期，我们往往可以先快速的拿来用，而不用在意其细节。那么，最简单的办法就是直接修改套用mxnet中的示例。
示例主要集中在/example/image-classification和/tests/python当中。
下面就简单的介绍如何快速开始训练： 

（1）新建一个文件夹，将/example/image-classification文件夹下
     find_mxnet.py、symbol_alexnet.py、train_imagenet.py和train_model.py这4个文件copy进来；
（2）保持find_mxnet.py文件不变，在symbol_alexnet.py中按照里面Alexnet网络的构建方法来建立自己的网络。
    最后最好再把文件名改为自己的网络名，如mynet.py;
（3）将train_imagenet.py最好也修改一下名字，如改为train_xx.py.将第40行代码中的’symbol’删除，
     否则你的网络文件就必须以‘symbol’开始命名，如symbol_mynet.py.

net = importlib.import_module('symbol_' + args.network).get_symbol(args.num_classes)
改为：
net = importlib.import_module(args.network).get_symbol(args.num_classes)


剩下的就是要在parser.add_argument中修改默认值了，否则你在调用mynet.py就必须准确的指定每一个参数。
（4）写一个脚本文件，运行脚本文件就可以开始训练了。如

python train_xx.py --data-dir=xx --model-prefix==xx --gpus=0
# 如果想要在每次epoch结束时保存一个模型，那么model-prefix就不能为空




12  利用mxnet 从零开始码一个皮卡丘检测器-CNN目标检测入门教程 SSD算法 SSD (Single-Shot MultiBox Object Detection).
https://zhuanlan.zhihu.com/p/28867241
https://zhuanlan.zhihu.com/gluon



目标检测通俗的来说是为了找到图像或者视频里的所有目标物体。在下面这张图中，两狗一猫的位置，包括它们所属的类（狗／猫），需要被正确的检测到。


所以和图像分类不同的地方在于，目标检测需要找到尽量多的目标物体，而且要准确的定位物体的位置，一般用矩形框来表示。

在接下来的章节里，我们先介绍一个流行的目标检测算法，SSD (Single-Shot MultiBox Object Detection).


SSD: Single Shot MultiBox Detector

顾名思义，算法的核心是用卷积神经网络一次前向推导求出大量多尺度（几百到几千）
的方框来表示目标检测的结果。网络的结构用下图表示。

跟所有的图像相关的网络一样，我们需要一个主干网络来提取特征，同时也是作为第一个预测特征层。
网络在当前层产生大量的预设框，和与之对应的每个方框的分类概率（背景，猫，狗等等）
以及真正的物体和预设框的偏移量。在完成当前层的预测后，我们会下采样当前特征层，
作为新的预测层，重新产生新的预设框，分类概率，偏移量。这个过程往往会重复好几次，
直到预测特征层到达全局尺度（ 1 \times 1 )。

接下来我们用例子解释每个细节实现。


【1】预设框 Default anchor boxes

预设框的形状和大小可以由参数控制，我们往往设置一堆预设框，
以期望任意图像上的物体都能有一个预设框能大致重合，由于每个预设框需要对应的预测网络预测值，
所以希望对于每个物体都有100%重合的预设框是不现实的，可能会需要几十万甚至几百万的预设框，
但是采样的预设框越多，重合概率越好，用几千到上万个预设框基本能实现略大于70%的最好重合率，
同时保证了检测的速度。

为了保证重合覆盖率，对于每个特征层上的像素点，我们用不同的大小和长宽比来采样预设框。 
假设在某个特定的特征层（ w × h ），每个预设框的中心点就是特征像素点的中心，
然后我们用如下的公式采样预设框：

预设框的中心为特征像素点的中心 

x =(i+0.5)/w , y=(j+0.5)/h   i：[0：w） j:[0:h)
对于长宽比 r = 1 , 尺寸s: (0,1] , 生成的预设框大小 ws * hs 

这里例子里，我们用事先实现的层MultiBoxPrior产生预设框，输入 n 个预设尺寸，
和 m 个预设的长宽比，输出为 n+m-1 个方框而不是 n \times m 个。
 当然我们完全可以使用其他的算法产生不同的预设框，
但是实践中我们发现上述的方法覆盖率和相应需要的预设框数量比较合适。


import mxnet as mx
from mxnet import nd
from mxnet.contrib.ndarray import MultiBoxPrior

n = 40
# 输入形状: batch x channel x height x weight
x = nd.random_uniform(shape=(1, 3, n, n))  

y = MultiBoxPrior(x, sizes=[.5, .25, .1], ratios=[1, 2, .5])

# 取位于 (20,20) 像素点的第一个预设框
# 格式为 (x_min, y_min, x_max, y_max)
boxes = y.reshape((n, n, -1, 4))
print('The first anchor box at row 21, column 21:', boxes[20, 20, 0, :])

import matplotlib.pyplot as plt
    """convert an anchor box to a matplotlib rectangle"""
def box_to_rect(box, color, linewidth=3):
    box = box.asnumpy()
    return plt.Rectangle(
        (box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]),
        fill=False, edgecolor=color, linewidth=linewidth)
colors = ['blue', 'green', 'red', 'black', 'magenta']
plt.imshow(nd.ones((n, n, 3)).asnumpy())
anchors = boxes[20, 20, :, :]
for i in range(anchors.shape[0]):
    plt.gca().add_patch(box_to_rect(anchors[i,:]*n, colors[i]))
plt.show()




【2】分类预测 Predict classes

这个部分的目标很简单，就是预测每个预设框对应的分类。
我们用 3 × 3 ， Padding (填充) 1 × 1 的卷积来预测分类概率，
这样可以做到卷积后空间上的形状不会变化，而且由于卷积的特点，
每个卷积核会扫过每个预测特征层的所有像素点，得到 c × h  × w 个预测值， 
h × w 对应了空间上所有的像素点，每个通道对应特定的预设框 c = (N_{class} + 1) × N_{anchors} 。
N_{class} + 1) 所有类别+1个背景类。
假设有10个正类，每个像素5个预设框，那么我们就需要 11 × 5 = 55 个通道。 
具体的来说，对于每个像素第 i 个预设框：

    通道 i × (N_{class} + 1) 的值对应背景（非物体）的得分
    通道 i × (N_{class} + 1) + 1 + j 对应了第 j 类的得分

from mxnet.gluon import nn
def class_predictor(num_anchors, num_classes):
    """return a layer to predict classes"""
    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)

cls_pred = class_predictor(5, 10)
cls_pred.initialize()
x = nd.zeros((2, 3, 20, 20))
print('Class prediction', cls_pred(x).shape)



【3】预测预设框偏移 Predict anchor boxes
为了找到物体准确的位置，光靠预设框本身是不行的，我们还需要预测偏移量以便把真正的物体框出来。

假设 b 是某个预设框， Y 是目标物体的真实矩形框，我们需要预测的偏移为 
\Delta(Y, b) = (t_x, t_y, t_{width}, t_{height}) ，全都是长度为4的向量， 我们求得偏移

    t_x = (Y_x - b_x) / b_{width}
    t_y = (Y_y - b_y) / b_{height}
    t_{width} = (Y_{width} - b_{width}) / b_{width}
    t_{height} = (Y_{height} - b_{height}) / b_{height} 
所有的偏移量都除以预设框的长或宽是为了更好的收敛。

类似分类概率，我们同样用 3 × 3 填充 1 × 1 的卷积来预测偏移。
这次不同的是，对于每个预设框，我们只需要 4 个通道来预测偏移量， 
一共需要 N_{anchors} × 4 个通道，
第 i 个预设框对应的偏移量存在通道 i × 4 到通道 i × 4 + 3 之间。

def box_predictor(num_anchors):
    """return a layer to predict delta locations"""
    return nn.Conv2D(num_anchors * 4, 3, padding=1)

box_pred = box_predictor(10)
box_pred.initialize()
x = nd.zeros((2, 3, 20, 20))
print('Box prediction', box_pred(x).shape)

>>>>   Box prediction (2, 40, 20, 20)


【4】下采样特征层 Down-sample features

每次我们下采样特征层到一半的长宽，用Pooling(池化)操作就可以轻松的做到，
当然也可以用stride(步长)为2的卷积直接得到。在下采样之前，
我们会希望增加几层卷积层作为缓冲，防止特征值对应多尺度带来的混乱，
同时又能增加网络的深度，得到更好的抽象。
"""stack two Conv-BatchNorm-Relu blocks and then a pooling layer to halve the feature size"""
def down_sample(num_filters):
    out = nn.HybridSequential()
    for _ in range(2):
        out.add(nn.Conv2D(num_filters, 3, strides=1, padding=1))##卷积
        out.add(nn.BatchNorm(in_channels=num_filters))##正则化
        out.add(nn.Activation('relu'))##激活
    out.add(nn.MaxPool2D(2))##最大值池化
    return out

blk = down_sample(10)
blk.initialize()
x = nd.zeros((2, 3, 20, 20))
print('Before', x.shape, 'after', blk(x).shape)

>>> Before (2, 3, 20, 20) after (2, 10, 10, 10)




【5】整合多个特征层预测值 Manage predictions from multiple layers

SSD算法的一个关键点在于它用到了多尺度的特征层来预测不同大小的物体。
相对来说，浅层的特征层的空间尺度更大，越到网络的深层，空间尺度越小，
最后我们往往下采样直到 1 × 1 ，用来预测全图大小的物体。
所以每个特征层产生的预设框， 分类概率，框偏移量需要被整合起来统一在全图与真实的物体比较。
为了做到一一对应，我们统一把所有的预设框， 分类概率，框偏移量 平铺再连接。
得到的是按顺序排列但是摊平的所有预测值和预设框。


# 随便创建一个大小为 20x20的预测层
feat1 = nd.zeros((2, 8, 20, 20))
print('Feature map 1', feat1.shape)
cls_pred1 = class_predictor(5, 10)
cls_pred1.initialize()
y1 = cls_pred1(feat1)
print('Class prediction for feature map 1', y1.shape)
# 下采样
ds = down_sample(16)
ds.initialize()
feat2 = ds(feat1)
print('Feature map 2', feat2.shape)
cls_pred2 = class_predictor(3, 10)
cls_pred2.initialize()
y2 = cls_pred2(feat2)
print('Class prediction for feature map 2', y2.shape)


我们统一把所有的预设框， 分类概率，框偏移量 平铺再连接。

def flatten_prediction(pred):
    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))

def concat_predictions(preds):
    return nd.concat(*preds, dim=1)

flat_y1 = flatten_prediction(y1)
print('Flatten class prediction 1', flat_y1.shape)
flat_y2 = flatten_prediction(y2)
print('Flatten class prediction 2', flat_y2.shape)
print('Concat class predictions', concat_predictions([flat_y1, flat_y2]).shape)

我们总是确保在 dim = 1 上连接，以免打乱一一对应的关系。


【6】主干网络 Body network

主干网络用来从原始图像输入提取特征。 
一般来说我们会用预先训练好的用于分类的高性能网络（VGG, ResNet等）来提取特征。
在这里我们就简单地堆叠几层卷积和下采样层作为主干网络的演示。


rom mxnet import gluon
def body():
    """return the body network"""
    out = nn.HybridSequential()
    for nfilters in [16, 32, 64]:
        out.add(down_sample(nfilters))
    return out

bnet = body()
bnet.initialize()
x = nd.zeros((2, 3, 256, 256))
print('Body network', [y.shape for y in bnet(x)])

>>> Body network [(64, 32, 32), (64, 32, 32)]


【7】设计一个简单的SSD示意网络 Create a toy SSD model

我们这里介绍一个示意用的简单SSD网络，出于速度的考量，输入图像尺寸定为 256 \times 256 。
def toy_ssd_model(num_anchors, num_classes):
    """return SSD modules"""
    downsamples = nn.Sequential()
    class_preds = nn.Sequential()
    box_preds = nn.Sequential()

    downsamples.add(down_sample(128))
    downsamples.add(down_sample(128))
    downsamples.add(down_sample(128))

    for scale in range(5):
        class_preds.add(class_predictor(num_anchors, num_classes))
        box_preds.add(box_predictor(num_anchors))

    return body(), downsamples, class_preds, box_preds

print(toy_ssd_model(5, 2))


【8】网络前向推导 Forward

既然我们已经设计完网络结构了，接下来可以定义网络前向推导的步骤。

首先得到主干网络的输出，然后对于每一个特征预测层，推导当前层的预设框，
分类概率和偏移量。最后我们把这些输入摊平，连接，作为网络的输出。

def toy_ssd_forward(x, body, downsamples, class_preds, box_preds, sizes, ratios):                
    # 计算主干网络的输出        
    x = body(x)

    # 在每个预测层, 计算预设框，分类概率，偏移量
    # 然后在下采样到下一层预测层，重复
    default_anchors = []
    predicted_boxes = []  
    predicted_classes = []

    for i in range(5):
        default_anchors.append(MultiBoxPrior(x, sizes=sizes[i], ratios=ratios[i]))
        predicted_boxes.append(flatten_prediction(box_preds[i](x)))
        predicted_classes.append(flatten_prediction(class_preds[i](x)))
        if i < 3:
            x = downsamples[i](x)
        elif i == 3:
            # 最后一层可以简单地用全局Pooling
            x = nd.Pooling(x, global_pool=True, pool_type='max', kernel=(4, 4))

    return default_anchors, predicted_classes, predicted_boxes


【9】打包收工 Put all things together

from mxnet import gluon
class ToySSD(gluon.Block):
    def __init__(self, num_classes, **kwargs):
        super(ToySSD, self).__init__(**kwargs)
        # 5个预测层，每层负责的预设框尺寸不同，由小到大，符合网络的形状
        self.anchor_sizes = [[.2, .272], [.37, .447], [.54, .619], [.71, .79], [.88, .961]]
        # 每层的预设框都用 1，2，0.5作为长宽比候选
        self.anchor_ratios = [[1, 2, .5]] * 5
        self.num_classes = num_classes

        with self.name_scope():
            self.body, self.downsamples, self.class_preds, self.box_preds = toy_ssd_model(4, num_classes)

    def forward(self, x):
        default_anchors, predicted_classes, predicted_boxes = toy_ssd_forward(x, self.body, self.downsamples,
            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)
        # 把从每个预测层输入的结果摊平并连接，以确保一一对应
        anchors = concat_predictions(default_anchors)
        box_preds = concat_predictions(predicted_boxes)
        class_preds = concat_predictions(predicted_classes)
        # 改变下形状，为了更方便地计算softmax
        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))

        return anchors, class_preds, box_preds


【10】网络输出示意 Outputs of ToySSD
# 新建一个2个正类的SSD网络
net = ToySSD(2)
net.initialize()
x = nd.zeros((1, 3, 256, 256))
default_anchors, class_predictions, box_predictions = net(x)
print('Outputs:', 'anchors', default_anchors.shape, 'class prediction', class_predictions.shape, 'box prediction', box_predictions.shape)


【11】数据集 Dataset

聊了半天怎么构建一个虚无的网络，接下来看看真正有意思的东西。

我们用3D建模批量生成了一个皮卡丘的数据集，产生了1000张图片作为这个展示用的训练集。
这个数据集里面，皮神会以各种角度，各种姿势出现在各种背景图中，就像Pokemon Go里增强现实那样炫酷。
因为是生成的数据集，我们自然可以得到每只皮神的真实坐标和大小，用来作为训练的真实标记。


【11】下载数据集 Download dataset
下载提前准备好的数据集并验证

from mxnet.test_utils import download
import os.path as osp
def verified(file_path, sha1hash):
    import hashlib
    sha1 = hashlib.sha1()
    with open(file_path, 'rb') as f:
        while True:
            data = f.read(1048576)
            if not data:
                break
            sha1.update(data)
    matched = sha1.hexdigest() == sha1hash
    if not matched:
        print('Found hash mismatch in file {}, possibly due to incomplete download.'.format(file_path))
    return matched

url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/datasets/pikachu/{}'
hashes = {'train.rec': 'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8',
          'train.idx': 'dcf7318b2602c06428b9988470c731621716c393',
          'val.rec': 'd6c33f799b4d058e82f2cb5bd9a976f69d72d520'}
for k, v in hashes.items():
    fname = 'pikachu_' + k
    target = osp.join('data', fname)
    url = url_format.format(k)
    if not osp.exists(target) or not verified(target, v):
        print('Downloading', target, url)
        download(url, fname=fname, dirname='data', overwrite=True)



【12】
加载数据 Load dataset

加载数据可以用mxnet.image.ImageDetIter，
同时还提供了大量数据增强的选项，比如翻转，随机截取等等。

import mxnet.image as image
data_shape = 256
batch_size = 32
def get_iterators(data_shape, batch_size):
    class_names = ['pikachu']
    num_class = len(class_names)
    train_iter = image.ImageDetIter(
        batch_size=batch_size,
        data_shape=(3, data_shape, data_shape),
        path_imgrec='./data/pikachu_train.rec',
        path_imgidx='./data/pikachu_train.idx',
        shuffle=True,
        mean=True,
        rand_crop=1,
        min_object_covered=0.95,
        max_attempts=200)
    val_iter = image.ImageDetIter(
        batch_size=batch_size,
        data_shape=(3, data_shape, data_shape),
        path_imgrec='./data/pikachu_val.rec',
        shuffle=False,
        mean=True)
    return train_iter, val_iter, class_names, num_class

train_data, test_data, class_names, num_class = get_iterators(data_shape, batch_size)
batch = train_data.next()
print(batch

