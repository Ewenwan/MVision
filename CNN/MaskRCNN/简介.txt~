https://zhuanlan.zhihu.com/p/25954683

代码
http://blog.csdn.net/zhangjunhit/article/details/64920075?locationNum=6&fps=1

http://baijiahao.baidu.com/s?id=1582335647216720222&wfr=spider&for=pc

github代码
https://github.com/TuSimple/mx-maskrcnn

Mask-RCNN implementation in MXNet
https://github.com/xilaili/maskrcnn.mxnet

TuSimple
GitHub: https://github.com/TuSimple/mx-maskrcnn 


1 简述
Mask R-CNN是一个小巧、灵活的通用对象实例分割框架（object instance segmentation）。
它不仅可对图像中的目标进行检测，还可以对每一个目标给出一个高质量的分割结果。
它在Faster R-CNN[1]基础之上进行扩展，并行地在bounding box recognition分支上
添加一个用于预测目标掩模（object mask）的新分支。

该网络还很容易扩展到其他任务中，比如估计人的姿势，也就是关键点识别（person keypoint detection）。
该框架在COCO的一些列挑战任务重都取得了最好的结果，包括实例分割（instance segmentation）、
候选框目标检测（bounding-box object detection）和人关键点检测（person keypoint detection）。

2 结构
一共可以分为两个分支：
（1）第一个分支为原始Faster R-CNN的结构，它用于对候选窗口进行分类和窗口坐标回归。
（2）第二个分支对每一个感兴趣区域（Region of Interest，RoI）预测分割掩模，
它利用了一个小的全卷积网络结构[2]（Fully Convolutional Network，FCN）。
3 主要关键因素
关键点1：
解决特征图与原始图像上的RoI不对准问题问题描述：RoIPool[3]
是一种针对每一个RoI的提取一个小尺度特征图（e.g. 7x7）的标准操作，
它用以解决将不同尺度的RoI提取成相同尺度的特征大小的问题。
RoIPool首先将浮点数值的RoI量化成离散颗粒的特征图，
然后将量化的RoI分成几个空间的小块（spatial bins），
最后对每个小块进行max pooling操作生成最后的结果。
该操作类似于下图，但只取了一个尺度。通过计算[x/16]在连续坐标x上进行量化，
其中16是特征图的步长，[ . ]表示四舍五入。这些量化引入了RoI与提取到的特征的不对准问题。
由于分类问题对平移问题比较鲁棒，所以影响比较小。但是这在预测像素级精度的掩模时会产生一个非常的大的负面影响。
解决方案：作者提出RoIAlign层来解决这个问题，并且将提取到的特征与输入对齐。
方法很简单，避免对RoI的边界或者块（bins）做任何量化，例如直接使用x/16代替[x/16]。
作者使用双线性插值（bilinear interpolation）在每个RoI块中4个采样位置上计算输入特征的精确值，并将结果聚合（使用max或者average）。

关键点2：
将掩模预测和分类预测拆解该框架对每个类别独立地预测一个二值掩模，没有引入类间竞争，
每个二值掩模的类别依靠网络RoI分类分支给出的分类预测结果。
这与FCNs不同，FCNs是对每个像素进行多类别分类，它同时进行分类和分割，
基于实验结果表明这样对于对象实例分割会得到一个较差的性能。
下面介绍一下更多的细节，在训练阶段，作者对于每个采样的RoI定义一个多任务损失函数，
前两项不过多介绍。掩模分支对于每个RoI会有一个维度的输出，它编码了K个分辨率为的二值掩模，
分别对应着K个类别。因此作者利用了a per-pixel sigmoid，
并且定义为平均二值交叉熵损失（the average binary cross-entropy loss）。
对于一个属于第k个类别的RoI，仅仅考虑第k个mask（其他的掩模输入不会贡献到损失函数中）。
这样的定义会允许对每个类别都会生成掩模，并且不会存在类间竞争。
关键点3：
掩模表示一个掩模编码了一个输入对象的空间布局。
作者使用了一个FCN来对每个RoI预测一个的掩模，这保留了空间结构信息。
对于以上几个关键点，
作者在表格2中给出了详细的性能比较。


